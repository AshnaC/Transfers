sudo apt update
sudo apt install python3-libnvinfer python3-libnvinfer-dev -y
sudo apt install python3-libnvinfer-plugin
python3 -c "import tensorrt as trt; print(trt.__version__)"



ls /usr/lib/python3*/dist-packages/tensorrt
export PYTHONPATH=$PYTHONPATH:/usr/lib/python3.8/dist-packages

trtexec --onnx=model_fp32.onnx \
        --saveEngine=model_fp16.engine \
        --fp16 \
        --shapes=input_0:1x3x224x224,input_1:1x3x224x224 \
        --workspace=4096

import tensorrt as trt
import pycuda.driver as cuda
import pycuda.autoinit
import numpy as np

INPUT_SHAPE = (1, 3, 224, 224)
OUTPUT_SIZE = 1000
TRT_LOGGER = trt.Logger(trt.Logger.WARNING)

# Load engine
engine_file_path = "model_fp16.engine"
with open(engine_file_path, "rb") as f, trt.Runtime(TRT_LOGGER) as runtime:
    engine = runtime.deserialize_cuda_engine(f.read())

context = engine.create_execution_context()

# Optional: set input shapes if dynamic
context.set_binding_shape(0, INPUT_SHAPE)
context.set_binding_shape(1, INPUT_SHAPE)

# Prepare host inputs
input_0 = np.random.rand(*INPUT_SHAPE).astype(np.float32)
input_1 = np.random.rand(*INPUT_SHAPE).astype(np.float32)
output = np.empty([1, OUTPUT_SIZE], dtype=np.float32)

# Device memory
d_input_0 = cuda.mem_alloc(input_0.nbytes)
d_input_1 = cuda.mem_alloc(input_1.nbytes)
d_output = cuda.mem_alloc(output.nbytes)

# Bindings (order must match engine)
bindings = [int(d_input_0), int(d_input_1), int(d_output)]

# Copy to device
stream = cuda.Stream()
cuda.memcpy_htod_async(d_input_0, input_0, stream)
cuda.memcpy_htod_async(d_input_1, input_1, stream)

# Inference
context.execute_async_v2(bindings=bindings, stream_handle=stream.handle)

# Copy result back
cuda.memcpy_dtoh_async(output, d_output, stream)
stream.synchronize()

print("âœ… Inference done")
print("Output shape:", output.shape)
print("First 10 values:", output[0, :10])
print("Predicted class:", np.argmax(output))

